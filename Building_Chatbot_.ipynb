{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMUNjPPhubxaf+bVJEcMTUU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataGuy-Kariuki/Whats-App-Data-Analysis-/blob/main/Building_Chatbot_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "A chatbot is a computer program that can simulate a conversation with a human user. Chatbots are often used to provide customer service, answer questions, or provide information. They can be built using a variety of programming languages and frameworks. One popular way to build a chatbot is to use a natural language processing (NLP) library. NLP libraries provide tools for understanding and generating human language."
      ],
      "metadata": {
        "id": "HV1SN3dOZALs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code explanations\n",
        "\n",
        "* The first line imports the nltk library.\n",
        "* This library contains a number of tools for natural language processing, including a tokenizer for splitting text into words and a stemmer for removing suffixes from words.\n",
        "\n",
        "* The second line downloads the punkt tokenizer from the nltk library. This tokenizer is used to split text into words.\n",
        "\n",
        "* The third line imports the PorterStemmer class from the nltk.stem module. This class is used to stem words by removing suffixes.\n",
        "\n",
        "* The fourth line creates an instance of the PorterStemmer class and assigns it to the stemmer variable.\n",
        "\n",
        "* The fifth line imports the tensorflow library. This library is used for machine learning.\n",
        "\n",
        "* The sixth line imports the numpy library. This library is used for scientific computing.\n",
        "\n",
        "* The seventh line imports the random library. This library is used for generating random numbers.\n",
        "\n",
        "* The eighth line imports the json library. This library is used for encoding and decoding JSON data."
      ],
      "metadata": {
        "id": "eRL9fwlBZc6f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGeMsdm5YpYQ"
      },
      "outputs": [],
      "source": [
        "# Libraries needed for NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Libraries needed for Tensorflow processing\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Intents.Json File"
      ],
      "metadata": {
        "id": "9jdF6_wBZ7qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the intents.json file from your local device\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "d3d58GmkZ6dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jq4eKgOuY_bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import our chat-bot intents file\n",
        "with open('/content/intents22.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "metadata": {
        "id": "TfgNq-eVaJ7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the intents file"
      ],
      "metadata": {
        "id": "j-DDNrp9acPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intents"
      ],
      "metadata": {
        "id": "N2L4OJjnaY40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first line creates an empty list called words.\n",
        "\n",
        "* The second line creates an empty list called classes.\n",
        "\n",
        "* The third line creates an empty list called documents.\n",
        "\n",
        "* The fourth line creates a list of characters that will be ignored when tokenizing the sentences. In this case, the only character that is ignored is the question mark (?).\n",
        "\n",
        "* The fifth line iterates through each intent in the intents dictionary."
      ],
      "metadata": {
        "id": "Hx-SEhZ-iFQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = ['?']\n",
        "# loop through each sentence in the intent's patterns\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each and every word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add word to the words list\n",
        "        words.extend(w)\n",
        "        # add word(s) to documents\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add tags to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "metadata": {
        "id": "pV-isyQ1aZdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first line performs stemming on each word in the words list, converts it to lowercase, and removes any duplicates. The stemmer.stem() function is used to perform stemming, and the .lower() method is used to convert the words to lowercase. The .set() function is used to remove any duplicates from the list.\n",
        "\n",
        "* The second line sorts the words list in alphabetical order.\n",
        "\n",
        "* The third line performs stemming and lowercasing on each word in the classes list, and removes any duplicates.\n",
        "\n",
        "* The fourth line prints the number of documents in the dataset.\n",
        "\n",
        "* The fifth line prints the number of classes in the dataset, as well as the list of classes.\n",
        "\n",
        "* The sixth line prints the number of unique stemmed words in the dataset."
      ],
      "metadata": {
        "id": "oAcxA_47hukt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stemming and lower each word as well as remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicate classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "metadata": {
        "id": "kJDap4WnhomI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first line creates an empty list called training.\n",
        "\n",
        "* The second line creates an empty list called output.\n",
        "\n",
        "* The third line creates an empty array called output_empty that contains the same number of elements as the number of classes.\n",
        "\n",
        "* The first line iterates through each word in the words list.\n",
        "\n",
        "* he second line checks if the current word is in the pattern_words list. If it is, the bag list appends a 1. If it is not, the bag list appends a 0.\n",
        "\n",
        "* The third line creates a list of zeros that is the same length as the number of classes.\n",
        "\n",
        "* The fourth line sets the corresponding element in the output_row list to 1 if the current document is in the current class.\n",
        "\n",
        "* The fifth line adds the current bag list and output_row list to the training list.\n",
        "\n",
        "* The first line shuffles the training list.\n",
        "\n",
        "* The second line converts the training list to a NumPy array.\n",
        "\n",
        "* The third line creates a list of the first column of the training array.\n",
        "\n",
        "* The fourth line creates a list of the second column of the training array"
      ],
      "metadata": {
        "id": "eatDCJHfis2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create training data\n",
        "training = []\n",
        "output = []\n",
        "# create an empty array for output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# create training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stemming each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is '1' for current tag and '0' for rest of other tags\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffling features and turning it into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# creating training lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "metadata": {
        "id": "hCB5gZjNiPw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first line creates a sequential model using the tf.keras.Sequential() function.\n",
        "\n",
        "* The second line adds a dense layer with 10 units to the model. The input shape of the layer is the length of the first column of the train_x array.\n",
        "\n",
        "* The third line adds another dense layer with 10 units to the model.\n",
        "\n",
        "* The fourth line adds a dense layer with the same number of units as the second column of the train_y array. The activation function of the layer is set to softmax.\n",
        "\n",
        "* The fifth line compiles the model using the tf.keras.optimizers.Adam() optimizer, the loss='categorical_crossentropy' loss function, and the metrics=['accuracy'] metrics."
      ],
      "metadata": {
        "id": "GiZQ2WNwjb1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(10,input_shape=(len(train_x[0]),)))\n",
        "model.add(tf.keras.layers.Dense(10))\n",
        "model.add(tf.keras.layers.Dense(len(train_y[0]), activation='softmax'))\n",
        "model.compile(tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FBAubXNKjQmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(np.array(train_x), np.array(train_y), epochs=1000, batch_size=8, verbose=1)\n",
        "model.save(\"model.pkl\")"
      ],
      "metadata": {
        "id": "7lQq4cxEjmfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first line imports the pickle module.\n",
        "\n",
        "* he second line uses the pickle.dump() function to save the words and classes lists to a file called training_data.\n",
        "* The open() function is used to open the file in binary mode."
      ],
      "metadata": {
        "id": "83N3TzF-kMVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump( {'words':words, 'classes':classes}, open( \"training_data\", \"wb\" ) )"
      ],
      "metadata": {
        "id": "Te0jmLfgkGc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first line imports the keras.models module.\n",
        "\n",
        "* The second line uses the load_model() function to load the model from the model.pkl file."
      ],
      "metadata": {
        "id": "soYW1V4RkddJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(\"model.pkl\")"
      ],
      "metadata": {
        "id": "_EeuXmvykYiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first line imports the pickle module.\n",
        "\n",
        "* The second line uses the pickle.load() function to load the data from the training_data file. The open() function is used to open the file in binary mode.\n",
        "\n",
        "* The third line assigns the words list to the words variable.\n",
        "\n",
        "* The fourth line assigns the classes list to the classes variable."
      ],
      "metadata": {
        "id": "58NjfMjokmcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# restoring all the data structures\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']"
      ],
      "metadata": {
        "id": "QKfkM0tIkjyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* The first line opens the intents.json file in read mode.\n",
        "\n",
        "* The second line loads the JSON data from the file into a dictionary called intents."
      ],
      "metadata": {
        "id": "xj2Z45AqlBGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/intents22.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "metadata": {
        "id": "QJQzrQ5kk0j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The clean_up_sentence() function takes a sentence as input and returns a list of stemmed words. The stemmer.stem() function is used to stem each word, which means removing the suffixes from the word.\n",
        "* The bow() function takes a sentence and a list of words as input and returns a list of 0s and 1s. The 0s and 1s represent whether or not the words in the list of words exist in the sentence."
      ],
      "metadata": {
        "id": "qE1Ca8xxlaBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenizing the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stemming each word\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# returning bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words):\n",
        "    # tokenizing the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # generating bag of words\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "    bag=np.array(bag)\n",
        "    return(bag)"
      ],
      "metadata": {
        "id": "oKhMotg6lLxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The ERROR_THRESHOLD constant is set to 0.30.\n",
        "\n",
        "* The classify() function takes a sentence as input and returns a list of tuples. Each tuple contains the index of the class and the probability of the sentence being in that class.\n",
        "\n",
        "* The first line generates probabilities from the model for the sentence. The bow() function is used to generate the bag of words for the sentence, and the model.predict() function is used to generate the probabilities.\n",
        "\n",
        "* The second line filters out predictions below the ERROR_THRESHOLD.\n",
        "\n",
        "* The third line sorts the predictions by strength of probability.\n",
        "\n",
        "* The fourth line creates an empty list called return_list.\n",
        "\n",
        "* The fifth line iterates through the sorted predictions.\n",
        "\n",
        "* The sixth line adds the index of the class and the probability of the sentence being in that class to the return_list.\n",
        "\n",
        "* The seventh line closes the for loop.\n",
        "\n",
        "* The eighth line returns the return_list."
      ],
      "metadata": {
        "id": "ggIjBWu0lprL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ERROR_THRESHOLD = 0.30\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    bag = bow(sentence, words)\n",
        "    results = model.predict(np.array([bag]))\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results[0]) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence):\n",
        "    results = classify(sentence)\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # a random response from the intent\n",
        "                    return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ],
      "metadata": {
        "id": "t5j3mInBleSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing my chatbot"
      ],
      "metadata": {
        "id": "FOC8txCDl9lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response('Where are you located?')"
      ],
      "metadata": {
        "id": "wvRZF8Ril1Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response('That is helpful')"
      ],
      "metadata": {
        "id": "3dqwcGrHmBwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response('Bye')"
      ],
      "metadata": {
        "id": "HCl08obKmEup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response(\"Who are you?\")"
      ],
      "metadata": {
        "id": "3TCBFCicmL83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response(\"Tell me more about Nairobi?\")"
      ],
      "metadata": {
        "id": "rXBt1ahhmSGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response (\"How to connect with Reuben?\")"
      ],
      "metadata": {
        "id": "QqMqJ9kHmX99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response(\"Can I get your links?\")"
      ],
      "metadata": {
        "id": "qoyWyOj1m5N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KMFNDrYEnA-W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}